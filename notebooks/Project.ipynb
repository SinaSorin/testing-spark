{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdfd3d2-e8b1-46bd-9d06-1e09459aa20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.37-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (1.24.4)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.31.0)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lxml>=4.9.1 (from yfinance)\n",
      "  Downloading lxml-5.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting appdirs>=1.4.4 (from yfinance)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2023.3.post1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.1.tar.gz (315 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.2/315.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.17.1.tar.gz (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.12.2)\n",
      "Collecting html5lib>=1.1 (from yfinance)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.11/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2023.7.22)\n",
      "Downloading yfinance-0.2.37-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m405.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m627.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Building wheels for collected packages: frozendict, peewee\n",
      "  Building wheel for frozendict (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for frozendict: filename=frozendict-2.4.1-cp311-cp311-linux_x86_64.whl size=15499 sha256=0f332428fa6d97828aacddb9e62651037d99778f940ead504d11980d0bb626b7\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/02/6e/a0/b90b693ddaaf2bde1efb47df2576175a1983aef24936f71694\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.1-cp311-cp311-linux_x86_64.whl size=272833 sha256=4e813761e7d51a2d9021ddafea13f87fa77c369d886476592b85d9a8135bb28f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/33/d2/ca/79b9807826bc7ef0b86a1ee28c372daaf073f9aa8756eedd7f\n",
      "Successfully built frozendict peewee\n",
      "Installing collected packages: peewee, multitasking, appdirs, lxml, html5lib, frozendict, yfinance\n",
      "Successfully installed appdirs-1.4.4 frozendict-2.4.1 html5lib-1.1 lxml-5.2.1 multitasking-0.0.11 peewee-3.17.1 yfinance-0.2.37\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a3ab67-4c5d-4b97-9ffb-6bef6ca0e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Price Prediction - Level 1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60832c83-5039-450a-a611-5c8af9174969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Fetch historical stock data\n",
    "stock_data = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2022-01-01\")\n",
    "\n",
    "# Convert the data to a Spark DataFrame\n",
    "sdf = spark.createDataFrame(stock_data.reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a020e081-6f17-41ef-99e0-3afe10d9632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ensure date is in the correct format and fill missing values if necessary\n",
    "sdf = sdf.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "# Example of filling missing values for 'Volume' with zero\n",
    "sdf = sdf.na.fill({\"Volume\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad919c9-aabd-474e-88a5-abc9f178202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, avg, col\n",
    "\n",
    "# Define a window spec for ordering by date\n",
    "windowSpec = Window.orderBy(\"Date\")\n",
    "\n",
    "# Lagged features: Previous day's closing price\n",
    "sdf = sdf.withColumn(\"Prev_Close\", lag(\"Close\").over(windowSpec))\n",
    "\n",
    "# Moving averages: 7-day and 30-day moving averages of the closing price\n",
    "sdf = sdf.withColumn(\"MA_7\", avg(\"Close\").over(windowSpec.rowsBetween(-6, 0)))\n",
    "sdf = sdf.withColumn(\"MA_30\", avg(\"Close\").over(windowSpec.rowsBetween(-29, 0)))\n",
    "\n",
    "# Drop rows with any null values that might have been created during feature engineering\n",
    "sdf = sdf.na.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f23974-3667-4dc1-bddc-7ad238ce3e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+-----------------+-----------------+\n",
      "|      Date|             Open|             High|              Low|            Close|        Adj Close|   Volume|       Prev_Close|             MA_7|            MA_30|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+-----------------+-----------------+\n",
      "|2020-01-03| 74.2874984741211| 75.1449966430664|           74.125|74.35749816894531| 72.3491439819336|146322800| 75.0875015258789|74.72249984741211|74.72249984741211|\n",
      "|2020-01-06|73.44750213623047|74.98999786376953|          73.1875|74.94999694824219|72.92562866210938|118387200|74.35749816894531|74.79833221435547|74.79833221435547|\n",
      "|2020-01-07|74.95999908447266| 75.2249984741211|74.37000274658203|74.59750366210938|72.58265686035156|108872000|74.94999694824219|74.74812507629395|74.74812507629395|\n",
      "|2020-01-08|74.29000091552734|76.11000061035156|74.29000091552734|75.79750061035156|73.75025177001953|132079200|74.59750366210938|74.95800018310547|74.95800018310547|\n",
      "|2020-01-09|76.80999755859375|77.60749816894531|76.55000305175781|77.40750122070312|75.31675720214844|170108400|75.79750061035156| 75.3662503560384| 75.3662503560384|\n",
      "|2020-01-10| 77.6500015258789| 78.1675033569336|          77.0625| 77.5824966430664|75.48703002929688|140644800|77.40750122070312|75.68285696847099|75.68285696847099|\n",
      "|2020-01-13|77.91000366210938|79.26750183105469| 77.7874984741211|79.23999786376953|77.09976196289062|121532000| 77.5824966430664|76.27607073102679| 76.1274995803833|\n",
      "|2020-01-14|79.17500305175781|79.39250183105469| 78.0425033569336|78.16999816894531|76.05867767333984|161954400|79.23999786376953|76.82071358816964| 76.3544438680013|\n",
      "|2020-01-15| 77.9625015258789|           78.875|77.38749694824219|77.83499908447266|75.73272705078125|121923600|78.16999816894531|77.23285675048828|76.50249938964843|\n",
      "|2020-01-16|78.39749908447266|78.92500305175781|78.02249908447266|78.80999755859375|76.68138885498047|108829200|77.83499908447266| 77.8346415928432|76.71227195046164|\n",
      "|2020-01-17|79.06749725341797|79.68499755859375|            78.75|79.68250274658203|77.53031921386719|137816400|78.80999755859375|78.38964189801898|76.95979118347168|\n",
      "|2020-01-21|79.29750061035156|79.75499725341797|             79.0|79.14250183105469|77.00489807128906|110843200|79.68250274658203| 78.6374991280692|77.12769200251653|\n",
      "|2020-01-22| 79.6449966430664|79.99749755859375|79.32749938964844|79.42500305175781|77.27976989746094|101832400|79.14250183105469|78.90071432931083|77.29178564889091|\n",
      "|2020-01-23| 79.4800033569336|79.88999938964844| 78.9124984741211|79.80750274658203|77.65193939208984|104472000|79.42500305175781| 78.9817864554269|77.45950012207031|\n",
      "|2020-01-24|          80.0625| 80.8324966430664|79.37999725341797|79.57749938964844|77.42815399169922|146537600|79.80750274658203|79.18285805838448|77.59187507629395|\n",
      "|2020-01-27|77.51499938964844|77.94249725341797|76.22000122070312|77.23750305175781|75.15135192871094|161940000|79.57749938964844|79.09750148228237|77.57102966308594|\n",
      "|2020-01-28| 78.1500015258789| 79.5999984741211|78.04750061035156|79.42250061035156|77.27733612060547|162234000|77.23750305175781|79.18500191824776|77.67388916015625|\n",
      "|2020-01-29|81.11250305175781| 81.9625015258789|80.34500122070312|81.08499908447266|78.89492797851562|216229200|79.42250061035156|79.38535853794643|77.85342126143605|\n",
      "|2020-01-30|80.13500213623047|81.02249908447266|          79.6875|80.96749877929688| 78.7806167602539|126743200|81.08499908447266|79.64607238769531|78.00912513732911|\n",
      "|2020-01-31|80.23249816894531|80.66999816894531|77.07250213623047|77.37750244140625|75.28758239746094|199588400|80.96749877929688|79.35357230050224|77.97904786609467|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037405d4-1ad5-454e-8c11-8d1e6f98c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assuming your Spark DataFrame is named 'sdf' and has the following columns:\n",
    "# 'Open', 'High', 'Low', 'Close', 'Volume', 'Prev_Close', 'MA_7', 'MA_30'\n",
    "# where 'Close' is the target variable.\n",
    "\n",
    "# Define the columns that will be used as features\n",
    "featureCols = ['Open', 'High', 'Low', 'Prev_Close', 'MA_7', 'MA_30', 'Volume']\n",
    "\n",
    "# VectorAssembler to combine feature columns into a single vector column\n",
    "vecAssembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "\n",
    "# Prepare the data\n",
    "sdf = vecAssembler.transform(sdf)\n",
    "sdf = sdf.withColumnRenamed(\"Close\", \"label\")  # Rename 'Close' to 'label' as required by Spark ML\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainData, testData = sdf.randomSplit([0.7, 0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2082cc-c33c-4b11-89bd-5639bda4b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the models\n",
    "lrModel = lr.fit(trainData)\n",
    "gbtModel = gbt.fit(trainData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb2eac9-24c0-4809-82d3-f5f993a95df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE on test data = 0.995082\n",
      "GBT Regression RMSE on test data = 2.09486\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "lrPredictions = lrModel.transform(testData)\n",
    "gbtPredictions = gbtModel.transform(testData)\n",
    "\n",
    "# Evaluate the models\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "lrRMSE = evaluator.evaluate(lrPredictions)\n",
    "gbtRMSE = evaluator.evaluate(gbtPredictions)\n",
    "\n",
    "print(\"Linear Regression RMSE on test data = %g\" % lrRMSE)\n",
    "print(\"GBT Regression RMSE on test data = %g\" % gbtRMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d012df39-d047-4f7e-9e2c-d5854c3873b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>label</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Prev_Close</th>\n",
       "      <th>MA_7</th>\n",
       "      <th>MA_30</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>74.287498</td>\n",
       "      <td>75.144997</td>\n",
       "      <td>74.125000</td>\n",
       "      <td>74.357498</td>\n",
       "      <td>72.349144</td>\n",
       "      <td>146322800</td>\n",
       "      <td>75.087502</td>\n",
       "      <td>74.722500</td>\n",
       "      <td>74.722500</td>\n",
       "      <td>[74.2874984741211, 75.1449966430664, 74.125, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>73.447502</td>\n",
       "      <td>74.989998</td>\n",
       "      <td>73.187500</td>\n",
       "      <td>74.949997</td>\n",
       "      <td>72.925629</td>\n",
       "      <td>118387200</td>\n",
       "      <td>74.357498</td>\n",
       "      <td>74.798332</td>\n",
       "      <td>74.798332</td>\n",
       "      <td>[73.44750213623047, 74.98999786376953, 73.1875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>74.959999</td>\n",
       "      <td>75.224998</td>\n",
       "      <td>74.370003</td>\n",
       "      <td>74.597504</td>\n",
       "      <td>72.582657</td>\n",
       "      <td>108872000</td>\n",
       "      <td>74.949997</td>\n",
       "      <td>74.748125</td>\n",
       "      <td>74.748125</td>\n",
       "      <td>[74.95999908447266, 75.2249984741211, 74.37000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>74.290001</td>\n",
       "      <td>76.110001</td>\n",
       "      <td>74.290001</td>\n",
       "      <td>75.797501</td>\n",
       "      <td>73.750252</td>\n",
       "      <td>132079200</td>\n",
       "      <td>74.597504</td>\n",
       "      <td>74.958000</td>\n",
       "      <td>74.958000</td>\n",
       "      <td>[74.29000091552734, 76.11000061035156, 74.2900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>76.809998</td>\n",
       "      <td>77.607498</td>\n",
       "      <td>76.550003</td>\n",
       "      <td>77.407501</td>\n",
       "      <td>75.316757</td>\n",
       "      <td>170108400</td>\n",
       "      <td>75.797501</td>\n",
       "      <td>75.366250</td>\n",
       "      <td>75.366250</td>\n",
       "      <td>[76.80999755859375, 77.60749816894531, 76.5500...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      label  Adj Close  \\\n",
       "0  2020-01-03  74.287498  75.144997  74.125000  74.357498  72.349144   \n",
       "1  2020-01-06  73.447502  74.989998  73.187500  74.949997  72.925629   \n",
       "2  2020-01-07  74.959999  75.224998  74.370003  74.597504  72.582657   \n",
       "3  2020-01-08  74.290001  76.110001  74.290001  75.797501  73.750252   \n",
       "4  2020-01-09  76.809998  77.607498  76.550003  77.407501  75.316757   \n",
       "\n",
       "      Volume  Prev_Close       MA_7      MA_30  \\\n",
       "0  146322800   75.087502  74.722500  74.722500   \n",
       "1  118387200   74.357498  74.798332  74.798332   \n",
       "2  108872000   74.949997  74.748125  74.748125   \n",
       "3  132079200   74.597504  74.958000  74.958000   \n",
       "4  170108400   75.797501  75.366250  75.366250   \n",
       "\n",
       "                                            features  \n",
       "0  [74.2874984741211, 75.1449966430664, 74.125, 7...  \n",
       "1  [73.44750213623047, 74.98999786376953, 73.1875...  \n",
       "2  [74.95999908447266, 75.2249984741211, 74.37000...  \n",
       "3  [74.29000091552734, 76.11000061035156, 74.2900...  \n",
       "4  [76.80999755859375, 77.60749816894531, 76.5500...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sdf.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e893802-68b1-4d1e-808c-9fb40bbdfbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Linear Regression RMSE on test data = 0.848373\n",
      "Scikit-learn GBT Regression RMSE on test data = 1.31216\n"
     ]
    }
   ],
   "source": [
    "# Note: This is a conceptual guideline for benchmarking. Actual implementation will vary based on your setup.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as SklearnGBR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assume 'df' is a pandas DataFrame equivalent of the Spark DataFrame 'sdf' used earlier\n",
    "\n",
    "X = df[featureCols]\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "lr_sk = SklearnLR().fit(X_train, y_train)\n",
    "gbt_sk = SklearnGBR().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions_sk = lr_sk.predict(X_test)\n",
    "gbt_predictions_sk = gbt_sk.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "lr_rmse_sk = np.sqrt(mean_squared_error(y_test, lr_predictions_sk))\n",
    "gbt_rmse_sk = np.sqrt(mean_squared_error(y_test, gbt_predictions_sk))\n",
    "\n",
    "print(\"Scikit-learn Linear Regression RMSE on test data = %g\" % lr_rmse_sk)\n",
    "print(\"Scikit-learn GBT Regression RMSE on test data = %g\" % gbt_rmse_sk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a54c390-3efa-4103-9a29-7f8d6706be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with a 1-second batch interval\n",
    "ssc = StreamingContext(spark.sparkContext,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14bc7fa9-a369-4812-8ac9-b3a7b784eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "import random\n",
    "\n",
    "# Simulate stock data stream\n",
    "def simulate_stock_data():\n",
    "    # Simulating stock data; in practice, this would come from your real-time data source\n",
    "    return [(random.uniform(100, 200), random.uniform(100, 200), random.uniform(100, 200), random.uniform(100, 200), random.randint(1000, 10000)) for _ in range(100)]\n",
    "\n",
    "# Create a DStream that simulates incoming stock data\n",
    "# Each RDD in the DStream represents data received in one batch interval\n",
    "stock_data_dstream = ssc.queueStream([ssc.sparkContext.parallelize(simulate_stock_data()) for _ in range(5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef90877-878b-4a61-bfd4-fc51ea51fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def process_time_batch(rdd: RDD):\n",
    "    if not rdd.isEmpty():\n",
    "        # Convert RDD to DataFrame\n",
    "        df = rdd.map(lambda x: Row(Open=x[0], High=x[1], Low=x[2], Close=x[3], Volume=x[4])).toDF()\n",
    "        \n",
    "        # Apply the same feature engineering as used for training the model\n",
    "        # This should include creating features like lagged variables, moving averages, etc.\n",
    "        # Here, for simplicity, let's assume the model was trained directly on these features without additional engineering\n",
    "        feature_df = vecAssembler.transform(df)\n",
    "        \n",
    "        # Predict using the model\n",
    "        predictions = lrModel.transform(feature_df)\n",
    "        \n",
    "        # Show predictions\n",
    "        predictions.select(\"prediction\").show()\n",
    "\n",
    "# Apply processing to each time batch\n",
    "stock_data_dstream.foreachRDD(process_time_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "215f5da2-389f-47e9-ad25-f3191599b4cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o377.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 71, in call\n    r = self.func(t, *rdds)\n        ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 236, in func\n    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n           ^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_3668/3059495156.py\", line 12, in process_time_batch\n    feature_df = vecAssembler.transform(df)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 262, in transform\n    return self._transform(dataset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 398, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sparkSession)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.IllegalArgumentException: Prev_Close does not exist. Available: Open, High, Low, Close, Volume\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ssc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTerminationOrTimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Run the streaming computation for 10 seconds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ssc\u001b[38;5;241m.\u001b[39mstop(stopSparkContext\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.py:254\u001b[0m, in \u001b[0;36mStreamingContext.awaitTerminationOrTimeout\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mawaitTerminationOrTimeout\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Wait for the execution to stop. Return `true` if it's stopped; or\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    throw the reported error during the execution; or `false` if the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m        time to wait in seconds\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTerminationOrTimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o377.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 71, in call\n    r = self.func(t, *rdds)\n        ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 236, in func\n    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n           ^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_3668/3059495156.py\", line 12, in process_time_batch\n    feature_df = vecAssembler.transform(df)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 262, in transform\n    return self._transform(dataset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 398, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sparkSession)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 185, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.IllegalArgumentException: Prev_Close does not exist. Available: Open, High, Low, Close, Volume\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(10)  # Run the streaming computation for 10 seconds\n",
    "ssc.stop(stopSparkContext=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3add1-0227-4f4c-acd5-468bf4b6cb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
